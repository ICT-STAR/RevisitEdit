alg_name: "GRACE"
model_name: "../hugging_cache/Llama-2-7b-chat"
device: 0

inner_params:
- model.layers[27].mlp.down_proj.weight

edit_lr: 1.0
n_iter: 50
eps: 1.0
dist_fn: euc # euc, mmd, cos
val_init: cold # cold, warm
val_train: sgd # sgd, pert
val_reg: None # early
reg: early_stop # early_stop
replacement: replace_last # replace_last, replace_all, replace_prompt
eps_expand: coverage # , moving_avg, decay
num_pert: 8 # only matters when using perturbation training
dropout: 0.0

# evaluation
# "traditional" for the evaluation of EasyEdit: context-free, teacher-forcing, ground truth length, match ratio
# "real-world" for the evaluation in our paper: context-guided, autoregressive decoding, natural stop criteria, LLM-as-a-Judge
evaluation_framework: "real-world"  
# for real-world evaluation in our paper, you need to set
context_type: "qa_inst"  # "qa_inst" for QA task instruction; "chat_temp" for chat model; default config is question-only
metric_type: "exact_match"  # "exact_match" or "llm_judge"
api_key: "xxxxx"  # Openai api key for llm judge (GPT-4o-mini)